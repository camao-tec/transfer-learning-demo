{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kC4c9mH275aQ"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/camao-tec/transfer-learning-demo/blob/main/transfer_learning_metal_defects.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6GPjsnj75aW",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgTYBL-N75aX",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuRcY-AB75aY",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Dataset download\n",
        " To get the dataset, follow these steps:\n",
        " 1. Open the dataset on kaggle: https://www.kaggle.com/datasets/kaustubhdikshit/neu-surface-defect-database\n",
        " 2. Click *Download* (login required) and save the zip archive\n",
        " 3. Unpack the zip archive and upload the `NEU-DET` folder into the `data` directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKlllMjo75aZ",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Brief overview of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5N6FQZZB75aZ",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "SIZE_OF_IMAGE = 112\n",
        "image_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((SIZE_OF_IMAGE, SIZE_OF_IMAGE)),\n",
        "])\n",
        "sample_folder = torchvision.datasets.ImageFolder('data/NEU-DET/train/images', transform=image_transforms, )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERDVI2Sn75aa",
        "outputId": "ce87552e-bfce-4eba-c433-0f6ee846697f",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Get the classes present in the dataset\n",
        "classes = sample_folder.classes\n",
        "\n",
        "# Plot three samples of each class\n",
        "fig, ax = plt.subplots(3, len(classes), figsize=(10, 5))\n",
        "\n",
        "for i in range(3):\n",
        "    for j, class_name in enumerate(classes):\n",
        "        idx = sample_folder.class_to_idx[class_name]\n",
        "        indices = torch.tensor(sample_folder.targets) == idx\n",
        "        sample_idx = torch.nonzero(indices)[i].item()\n",
        "        image = sample_folder[sample_idx][0]\n",
        "        \n",
        "        ax[i][j].imshow(image)\n",
        "        ax[i][j].axis('off')\n",
        "        if i == 0:\n",
        "            ax[i][j].set_title(class_name)\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urBMP-3o75ac",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### Load dataset for training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6wksge575ad",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 8\n",
        "SIZE_OF_IMAGE = 112\n",
        "\n",
        "prediction_trans = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((SIZE_OF_IMAGE, SIZE_OF_IMAGE)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(\n",
        "        mean=[0.5, 0.5, 0.5],\n",
        "        std=[0.5, 0.5, 0.5]\n",
        "    )\n",
        "])\n",
        "\n",
        "train_folder = torchvision.datasets.ImageFolder('data/NEU-DET/train/images', transform=prediction_trans, )\n",
        "test_folder = torchvision.datasets.ImageFolder('data/NEU-DET/validation/images', transform=prediction_trans, )\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_folder, shuffle=True, batch_size=BATCH_SIZE)\n",
        "test_loader = torch.utils.data.DataLoader(test_folder, shuffle=False, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZCCb32B75ad",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Model instantiation and adaption for specific use-case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9t8FtGX75ae",
        "outputId": "d5131c81-58bb-44a3-97d0-7fffa977518c",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# use the `cuda` device (GPU) if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRCCQXI175ae",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def get_transfermodel():\n",
        "    # 1) get pretrained model from the `torchvision` model library\n",
        "    model = torchvision.models.vgg19(weights=\"VGG19_Weights.IMAGENET1K_V1\").to(device)\n",
        "\n",
        "    # 2) freeze layers\n",
        "    for param in model.features.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # 3) adapt output for our specific task with 6 classes\n",
        "    num_classes = 6\n",
        "    num_features = model.classifier[6].in_features\n",
        "    model.classifier[6] = torch.nn.Linear(num_features, num_classes)\n",
        "    # ensure we retrain the entire classifier (all trainable parameters require gradients)\n",
        "    for param in model.classifier.parameters():\n",
        "        param.requires_grad = True\n",
        "    \n",
        "    return model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkGYmyjD75af",
        "outputId": "8b23e754-9af9-47be-e950-4bf2f6158291",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# let's have a look on the entire model architecture\n",
        "transfer_model = get_transfermodel()\n",
        "transfer_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcfj28KO75af",
        "outputId": "3027e4c8-2205-4085-945a-205c7a8ded46",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# let's have a look on the frozen parameters of the pretrained model\n",
        "for i, param in enumerate(transfer_model.features.parameters()):\n",
        "    print(f\"Param #{i:0>3d}: shape = {param.shape} , requires_grad = {param.requires_grad}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agbv54kO75ag",
        "outputId": "94d20192-f1db-46d0-ffa2-60a11b61d54e",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# let's have a look on the trainable parameters of output layers\n",
        "for i, param in enumerate(transfer_model.classifier.parameters()):\n",
        "    print(f\"Param #{i:0>3d}: shape = {param.shape} | requires_grad = {param.requires_grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DK1Y-uz875ag",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Model training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqKtHNZz75ah",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# create a directory for the models\n",
        "os.makedirs('models', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7ZQfvLF75ah",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def validate(model, valid_data, loss_fn):\n",
        "    valid_losses, valid_accuracies = [], []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in tqdm(valid_data, leave=False):\n",
        "            X_batch, y_batch = X_batch.to(device).float(), y_batch.to(device).long()\n",
        "            logits = model(X_batch)\n",
        "            loss = loss_fn(logits, y_batch)\n",
        "            valid_losses.append(loss.item())\n",
        "            preds = torch.argmax(logits, axis=1)\n",
        "            valid_accuracies.append(((preds == y_batch).sum() / len(preds)).item())\n",
        "    return np.mean(valid_losses), np.mean(valid_accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cvd7u23Z75ah",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def train(model, train_data, valid_data, loss_fn, opt, epoches, name):\n",
        "    train_losses, valid_losses = [], []\n",
        "    train_accuracies, valid_accuracies = [], []\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(opt, step_size=4, gamma=0.5)\n",
        "    \n",
        "    for epoch in tqdm(range(epoches)):\n",
        "        train_loss = []\n",
        "        train_acc = []\n",
        "        model.train()\n",
        "        for X_batch, y_batch in tqdm(train_data, leave=False):\n",
        "            opt.zero_grad()\n",
        "            X_batch, y_batch = X_batch.to(device).float(), y_batch.to(device).long()\n",
        "            logits = model(X_batch)\n",
        "            loss = loss_fn(logits, y_batch,)\n",
        "            train_loss.append(loss.item())\n",
        "\n",
        "            pred = torch.argmax(logits, dim=1)\n",
        "            train_acc.append(((pred == y_batch).sum() / len(pred)).item())\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        valid_loss, valid_accuracy = validate(model, valid_data, loss_fn)\n",
        "\n",
        "        train_accuracies.append(np.mean(train_acc))\n",
        "        train_losses.append(np.mean(train_loss))\n",
        "        valid_losses.append(valid_loss)\n",
        "        valid_accuracies.append(valid_accuracy)\n",
        "\n",
        "        print(f'epoch: {epoch}: train_loss: {np.mean(train_losses)}, train_acc: {np.mean(train_acc)}, val_loss: {valid_loss}, val_acc: {valid_accuracy}')\n",
        "        \n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': opt.state_dict(),\n",
        "            'loss': loss_fn,\n",
        "        }, f'models/{name}_{epoch}.pt')\n",
        "\n",
        "    return model, train_losses, train_accuracies, valid_losses, valid_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFvrklvX75ai",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# training setup and hyper parameters\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "opt = torch.optim.Adam(transfer_model.parameters(), lr=1e-5)\n",
        "EPOCHES = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "e7d36fd1a3b84e07bad3c38d4faad176",
            ""
          ]
        },
        "id": "AZ35b7zb75ai",
        "outputId": "2bb47353-52df-440f-b466-4c8c234e1699",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# train model\n",
        "transfer_model, train_losses, train_accuracies, valid_losses, valid_accuracies = train(\n",
        "    transfer_model, train_loader, test_loader, loss_fn, opt, epoches=EPOCHES, name=\"model_epoch\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IakojkEx75ai",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "def plot_losses_and_acc(train_losses, train_accuracies, valid_losses, valid_accuracies):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 10))\n",
        "    epochs = np.arange(1, len(train_losses) + 1)\n",
        "    axes[0].plot(epochs, train_losses)\n",
        "    axes[0].plot(epochs, valid_losses)\n",
        "    axes[0].set_title('Losses')\n",
        "    axes[0].legend(['Training', 'Validation'])\n",
        "    axes[0].set_xlim([1, len(train_losses)])\n",
        "\n",
        "    axes[1].plot(epochs, train_accuracies)\n",
        "    axes[1].plot(epochs, valid_accuracies)\n",
        "    axes[1].set_title('Accuracy')\n",
        "    axes[1].legend(['Training', 'Validation'])\n",
        "    axes[1].set_xlim([1, len(train_accuracies)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYLHIddI75aj",
        "outputId": "05f38304-9a02-4450-99b8-84ed77bfcd77",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "plot_losses_and_acc(train_losses, train_accuracies, valid_losses, valid_accuracies)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koQ5PgQp75aj",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## Model Prediction\n",
        "Now that we are satisfied with the model accuracy on the test data, let's see some predictions.\n",
        "Load the trained model from disk and predict the classes of some images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvtPQLVt75aj",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# load the model from disk\n",
        "best_model_epoch = 5\n",
        "checkpoint = torch.load(f'models/model_epoch_{best_model_epoch}.pt')\n",
        "model = get_transfermodel()\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.to(device) ;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCazc6lO75ak",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "SIZE_OF_IMAGE = 112\n",
        "image_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.Resize((SIZE_OF_IMAGE, SIZE_OF_IMAGE)),\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.Normalize(\n",
        "        mean=[0.5, 0.5, 0.5],\n",
        "        std=[0.5, 0.5, 0.5]\n",
        "    )\n",
        "])\n",
        "sample_folder_prediction = torchvision.datasets.ImageFolder('data/NEU-DET/validation/images', transform=image_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a1JJBmA75ak",
        "outputId": "1375464a-79a8-4e4d-e33d-2495fdb2aa56",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Get the classes present in the dataset\n",
        "classes = sample_folder_prediction.classes\n",
        "\n",
        "# Plot two samples with their corresponding label and prediction per class\n",
        "fig, ax = plt.subplots(len(classes), 2, figsize=(6, 16))\n",
        "\n",
        "for j in range(2):\n",
        "    for i, label_class_name in enumerate(classes):\n",
        "        idx = sample_folder_prediction.class_to_idx[label_class_name]\n",
        "        indices = torch.tensor(sample_folder_prediction.targets) == idx\n",
        "        sample_idx = torch.nonzero(indices)[j].item()\n",
        "        image = sample_folder_prediction[sample_idx][0]\n",
        "\n",
        "        image_input = image.unsqueeze(0).to(device).float()\n",
        "        logits = model(image_input)\n",
        "        prediction = torch.argmax(logits, dim=1).to(device=\"cpu\").squeeze().item()\n",
        "        prediction_class_name = classes[prediction]\n",
        "\n",
        "        image_transformed = image.permute(1, 2, 0).to(device=\"cpu\").numpy()\n",
        "        ax[i][j].imshow((image_transformed + 1) / 2)\n",
        "        ax[i][j].axis('off')\n",
        "        ax[i][j].set_title(f'Label = `{label_class_name}`\\nPrediction = `{prediction_class_name}`')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "5d41f672018807ff3d0df9e096a000603871dfca3531a9f7991b481c521db32b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
